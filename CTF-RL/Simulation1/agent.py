import numpy as np
import server as srv
import constants as const

class Agent():
    def __init__(self, nports, verbose=True):
        self.nports = nports  # Số lượng cổng có thể có
        self.Q = np.ones((nports+1, nports+1))  # Khởi tạo bảng Q-learning
        self.verbose = verbose  # Hiển thị thông tin chi tiết nếu đặt là True
    
    def set_learning_options(self, exploration=0.2, learningrate=0.1, discount=0.9, scanprobability=0.2):
        self.eps = exploration  # Xác suất khám phá
        self.alpha = learningrate  # Tỷ lệ học
        self.gamma = discount  # Tỷ lệ chiết khấu
        self.beta = scanprobability  # Xác suất scan
    
    def reset(self, env):
        self.env = env  # Môi trường server
        self.terminated = False  # Trạng thái kết thúc
        self.state = 0  # Trạng thái ban đầu
        self.steps = 0  # Số bước thực hiện
    
    def run_episode(self):
        _, _, self.terminated, s = self.env.reset()
        if self.verbose:
            print(s)
        
        while not self.terminated:
            self.step()
    
    def step(self):
        self.steps += 1
        
        action = self._select_action()
        
        msg = self._package_action_into_msg(action)
        response, reward, termination, s = self.env.step(msg)
        
        self._analyze_response(action, response, reward)
        self.terminated = termination
        if self.verbose:
            print(s)
    
    def _package_action_into_msg(self, action):
        if action == 0:
            return srv.Msg(const.SCAN, None)
        else:
            return srv.Msg(const.ATTACK, action)
    
    def _select_action(self):
        if np.random.random() < self.eps:
            if np.random.random() < self.beta:
                return 0  # Chọn scan
            else:
                return np.random.randint(1, self.nports+1)  # Chọn một cổng để tấn công
        else:
            return np.argmax(self.Q[self.state, :])  # Chọn hành động tối ưu dựa trên bảng Q
    
    def _analyze_response(self, action, response, reward):
        command = response.command
        
        if command == const.NONE:
            # No port was attacked, update Q-table with same state
            self._update_Q(self.state, self.state, action, reward)
            
        elif command == const.OPENPORT:
            # A port scan has found an open port, update Q-table with new state
            newstate = response.content
            self._update_Q(self.state, newstate, action, reward)
            self.state = newstate  # Move to the new state
            
        elif command == const.FLAG:
            # The correct port was attacked, update Q-table with the same state
            self._update_Q(self.state, self.state, action, reward)
    
    def _update_Q(self, oldstate, newstate, action, reward):
        # Find the best action for the new state by looking at the Q-values
        best_action_newstate = np.argmax(self.Q[newstate, :])
        
        # Update the Q-value using the Q-learning update rule
        self.Q[oldstate, action] = self.Q[oldstate, action] + self.alpha * (
            reward + self.gamma * self.Q[newstate, best_action_newstate] - self.Q[oldstate, action]
        )